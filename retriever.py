# vector_db.py

import os
import pickle
import hashlib
from glob import glob
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.prompts import PromptTemplate
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# ===== ì„¤ì • =====
pdf_path = "./papers/*.pdf"
persist_dir = "chroma_papers"
cache_path = "token_chunk.pkl"
hash_path = "pdf_hash.txt"


# ===== í•´ì‹œ ìƒì„± =====
def hash_files(files):
    h = hashlib.md5()
    for f in sorted(files):
        with open(f, "rb") as file:
            h.update(file.read())
    return h.hexdigest()


# ===== ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ (get í•¨ìˆ˜ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ) =====
db = None
retriever = None


def setup_vector_db():
    global db, retriever

    # ===== ë¬¸ì„œ ë¡œë”© ë° í† í°í™” =====
    pdf_files = glob(pdf_path)
    regenerate = False

    current_hash = hash_files(pdf_files)
    if os.path.exists(cache_path) and os.path.exists(hash_path):
        with open(hash_path, "r") as f:
            saved_hash = f.read()
        if saved_hash == current_hash:
            print("âœ… ìºì‹œëœ token_chunk ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
            with open(cache_path, "rb") as f:
                token_chunk = pickle.load(f)
        else:
            print("ğŸ“› PDF ë‚´ìš©ì´ ë³€ê²½ë¨. token_chunk ì¬ìƒì„± í•„ìš”.")
            regenerate = True
    else:
        regenerate = True

    if regenerate:
        print("ğŸ“„ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶„í•  ìˆ˜í–‰ ì¤‘...")
        all_papers = []
        for i, path_paper in enumerate(pdf_files):
            print(f"   â†’ ({i+1}/{len(pdf_files)}) {path_paper} ë¡œë”© ì¤‘...")
            loader = PyMuPDFLoader(path_paper)
            pages = loader.load()
            source = pages[0].metadata.get("source", path_paper)
            doc = Document(page_content="", metadata={"index": i, "source": source})
            for page in pages:
                doc.page_content += page.page_content
            all_papers.append(doc)

        token_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            encoding_name="cl100k_base",
            chunk_size=2000,
            chunk_overlap=200,
        )
        token_chunk = token_splitter.split_documents(all_papers)

        with open(cache_path, "wb") as f:
            pickle.dump(token_chunk, f)
        with open(hash_path, "w") as f:
            f.write(current_hash)
        print("âœ… token_chunk ì €ì¥ ì™„ë£Œ!")

    # ===== ì„ë² ë”© ëª¨ë¸ =====
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cuda"},
    )

    # ===== Chroma ë¡œë”© or ìƒì„± =====
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        db = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings,
        )
        print("âœ… ê¸°ì¡´ Chroma DB ë¡œë“œ ì™„ë£Œ")
    else:
        db = Chroma.from_documents(
            documents=token_chunk,
            embedding=embeddings,
            persist_directory=persist_dir,
            collection_metadata={"hnsw:space": "l2"},
        )
        print("ğŸ“¦ ìƒˆë¡œ ì„ë² ë”©í•˜ì—¬ Chroma DB ìƒì„± ì™„ë£Œ")

    retriever = db.as_retriever(search_kwargs={"k": 1})


def get_vector_db():
    return db


def get_retriever():
    return retriever


def get_compressor():
    from server import llm

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
        ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ì œê±°í•˜ì„¸ìš”.

        ë¬¸ì„œ:
        {context}

        ì§ˆë¬¸:
        {question}
        """,
    )

    return LLMChainExtractor.from_llm(llm, prompt=prompt)


def retriever_with_score(query):
    docs, scores = zip(*db.similarity_search_with_score(query))
    for doc, score in zip(docs, scores):
        doc.metadata["score"] = score
    return docs


def get_ensemble_retriever():
    # ê¸°ì¡´ ë²¡í„° retriever ì‚¬ìš©
    vector = get_retriever()

    # token_chunk ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(cache_path):
        with open(cache_path, "rb") as f:
            token_chunk = pickle.load(f)
    else:
        raise FileNotFoundError(
            "âŒ token_chunk.pkl not found. ë¨¼ì € setup_vector_db()ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        )

    # BM25 retriever ìƒì„±
    bm25 = BM25Retriever.from_documents(token_chunk)
    bm25.k = 3  # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ ì¡°ì ˆ ê°€ëŠ¥

    # Ensemble ìƒì„±
    return EnsembleRetriever(
        retrievers=[vector, bm25], weights=[0.7, 0.3]  # ë²¡í„° 70%, BM25 30%
    )


# ===== ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ìˆ˜í–‰ =====
if __name__ == "__main__":
    setup_vector_db()
    result = retriever_with_score("How does Exaone achieve good evaluation results?")
    for doc in result:
        print(doc.page_content[:100], "...", doc.metadata["score"])
