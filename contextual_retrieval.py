from langchain.prompts import ChatPromptTemplate
from server import llm
from langchain_core.output_parsers import StrOutputParser
from tqdm import tqdm
import os
import pickle
import hashlib
from glob import glob
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# ==== Context ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ====
context_prompt = ChatPromptTemplate(
    [
        (
            "user",
            """
         ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
         ì£¼ì–´ì§„ Document ì˜ ì¼ë¶€ì¸ Chunk ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ê´€ë ¨ì„± ìˆëŠ”
         ì§§ì€ ì„¤ëª…ì„ ìƒì„±í•˜ì„¸ìš”.
         
         # Input Format
         
         -[Document] : '<document>{document}</document>'
         -[Chunk] : '<chunk>{chunk}</chunk>'
         
         ì•„ë˜ì˜ ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬,
         ì´ ë¶€ë¶„ì— ëŒ€í•´ ê°„ê²°í•œ ì˜ë¬¸ Contextì„ ì‘ì„±í•˜ì„¸ìš”. (1-2ë¬¸ì¥)
         
         1. í…ìŠ¤íŠ¸ ë¶€ë¶„ì—ì„œ ë…¼ì˜ëœ ì£¼ìš” ì£¼ì œë‚˜ ê°œë…ì„ í¬í•¨í•˜ì„¸ìš”.
         2. ë¬¸ì„œ ì „ì²´ì˜ ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ì •ë³´ë‚˜ ë¹„êµë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.
         3. ê°€ëŠ¥í•œ ê²½ìš°, ì´ ì •ë³´ê°€ ë¬¸ì„œì˜ ì „ì²´ì ì¸ ì£¼ì œë‚˜ ëª©ì ê³¼ ì–´ë–»ê²Œ ì—°ê´€ë˜ëŠ” ì§€ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
         4. ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì£¼ìš” í•­ëª©ê³¼ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš”.
         
         í…ìŠ¤íŠ¸ ë¶€ë¶„ì˜ ê²€ìƒ‰ ì •í™•ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´,
         ë¬¸ì„œì˜ ì „ì²´ ë§¥ë½ì— í•´ë‹¹í•˜ëŠ” Context ë§Œì„ ì¶œë ¥í•˜ì„¸ìš”.
         ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
         
         Context :
         
         """,
        )
    ]
)
context_chain = context_prompt | llm | StrOutputParser()

# ===== ì„¤ì • =====
pdf_path = "./papers/*.pdf"
persist_dir = "chroma_papers_contextual_retrieval"
cache_path = "token_chunk.pkl"
hash_path = "pdf_hash.txt"


# ===== í•´ì‹œ ìƒì„± =====
def hash_files(files):
    h = hashlib.md5()
    for f in sorted(files):
        with open(f, "rb") as file:
            h.update(file.read())
    return h.hexdigest()


# ===== ì „ì—­ ë³€ìˆ˜ =====
db = None
ensemble_retriever = None


def setup_vector_db():
    global db, ensemble_retriever

    pdf_files = glob(pdf_path)
    regenerate = False

    current_hash = hash_files(pdf_files)
    if os.path.exists(cache_path) and os.path.exists(hash_path):
        with open(hash_path, "r") as f:
            saved_hash = f.read()
        if saved_hash == current_hash:
            regenerate = False
        else:
            print("ğŸ“› PDF ë‚´ìš©ì´ ë³€ê²½ë¨. token_chunk ì¬ìƒì„± í•„ìš”.")
            regenerate = True
    else:
        regenerate = True

    if regenerate:
        print("ğŸ“„ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶„í•  ìˆ˜í–‰ ì¤‘...")
        all_papers = []
        for i, path_paper in enumerate(pdf_files):
            print(f"   â†’ ({i+1}/{len(pdf_files)}) {path_paper} ë¡œë”© ì¤‘...")
            loader = PyMuPDFLoader(path_paper)
            pages = loader.load()
            source = pages[0].metadata.get("source", path_paper)
            doc = Document(page_content="", metadata={"index": i, "source": source})
            for page in pages:
                doc.page_content += page.page_content
            all_papers.append(doc)

        token_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            encoding_name="cl100k_base", chunk_size=2000, chunk_overlap=200
        )
        token_chunk = token_splitter.split_documents(all_papers)

        print("ğŸ§  ê° chunkì— context ìƒì„± ì¤‘...")
        for i, chunk in tqdm(enumerate(token_chunk), total=len(token_chunk)):
            doc = all_papers[chunk.metadata["index"]].page_content
            try:
                context = context_chain.invoke(
                    {"document": doc, "chunk": chunk.page_content}
                )
            except Exception as e:
                print(f"âŒ context ìƒì„± ì‹¤íŒ¨: chunk {i} â†’ {e}")
                context = ""
            token_chunk[i].page_content = context + "\n\n" + token_chunk[i].page_content

        with open(cache_path, "wb") as f:
            pickle.dump(token_chunk, f)
        with open(hash_path, "w") as f:
            f.write(current_hash)
        print("âœ… token_chunk ì €ì¥ ì™„ë£Œ!")
    else:
        print("âœ… ìºì‹œëœ token_chunk ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        with open(cache_path, "rb") as f:
            token_chunk = pickle.load(f)

    # ===== ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•­ìƒ í•„ìš”í•¨) =====
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cuda"},
    )

    # ===== Chroma ìƒì„± or ë¡œë”© =====
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
        print("âœ… ê¸°ì¡´ Chroma DB ë¡œë“œ ì™„ë£Œ")
    else:
        db = Chroma.from_documents(
            documents=token_chunk,
            embedding=embeddings,
            persist_directory=persist_dir,
            collection_metadata={"hnsw:space": "l2"},
        )
        print("ğŸ“¦ ìƒˆë¡œ ì„ë² ë”©í•˜ì—¬ Chroma DB ìƒì„± ì™„ë£Œ")

    # ===== Ensemble Retriever ìƒì„± =====
    bm25_retriever = BM25Retriever.from_documents(token_chunk)
    bm25_retriever.k = 2

    vector_retriever = db.as_retriever(search_kwargs={"k": 2})

    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5],
    )


def get_vector_db():
    return db


def get_retriever():
    return ensemble_retriever
