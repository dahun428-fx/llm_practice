from server import llm
from retriever import (
    get_vector_db,
    get_retriever,
    setup_vector_db,
    retriever_with_score,
    get_compressor,
)
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers import ContextualCompressionRetriever

import logging

setup_vector_db()
retriver = get_retriever()
prompt = ChatPromptTemplate(
    [
        (
            "user",
            """ë‹¹ì‹ ì€ QA(Question Answering) ì„ ìˆ˜í–‰í•˜ëŠ” Assistant ì…ë‹ˆë‹¤.
        ë‹¤ìŒì˜ Contextë¥¼ ì´ìš©í•˜ì—¬ Question ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
        ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        ë§Œì•½ ëª¨ë“  Context ë¥¼ ë‹¤ í™•ì¸í•´ë„ ì •ë³´ê°€ ì—†ë‹¤ë©´,
        "ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
        ---
        Context : {context}
        ---
        Question : {question}
        """,
        )
    ]
)
prompt.pretty_print()


def format_docs(docs):
    return "\n===\n".join(
        [doc.page_content + "\nURL" + doc.metadata["source"] for doc in docs]
    )


questions = [
    "Exaone ì–¸ì–´ ëª¨ë¸ì´ ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë‹¤ë¥¸ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "Phi-3 ì–¸ì–´ ëª¨ë¸ì€ ì–´ë–¤ ë°ì´í„°ë¡œ í•™ìŠµí–ˆë‚˜ìš”?",
    "Qwen 2 ì˜ ë‹¤êµ­ì–´ ì„±ëŠ¥ì€ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚¬ë‚˜ìš”?",
    "Gemma ì˜ ìŠ¤ëª° ëª¨ë¸ì€ ì–´ë–»ê²Œ í•™ìŠµí–ˆë‚˜ìš”?",
]

rewriter_prompt = PromptTemplate(
    template="""ë‹¹ì‹ ì€ AI ì–¸ì–´ ëª¨ë¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                                ì£¼ì–´ì§„ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼
                                ê²€ìƒ‰í•˜ê¸° ìœ„í•´ 3ê°€ì§€ ë‹¤ë¥¸ ì˜ë¬¸ ë²„ì „ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê²ƒì´
                                ë‹¹ì‹ ì˜ ì‘ì—…ì…ë‹ˆë‹¤.
                                ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì—¬ëŸ¬ ê´€ì ì„ ìƒì„±í•¨ìœ¼ë¡œì¨, 
                                ë‹¹ì‹ ì€ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ì„± ê²€ìƒ‰ì˜ í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆë„ë¡
                                ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ëŒ€ì²´ ì§ˆë¬¸ë“¤ì„
                                ìƒˆë¡œìš´ ì¤„ë¡œ êµ¬ë¶„í•˜ì—¬ ì œê³µí•˜ì„¸ìš”.
                                ---
                                ì›ë³¸ ì§ˆë¬¸ : {question}"""
)

mutli_query_retriever = MultiQueryRetriever.from_llm(
    retriever=retriver,
    llm=llm,
    prompt=rewriter_prompt,
)

# ğŸ”½ ì••ì¶•ê¸° ë¶ˆëŸ¬ì˜¤ê¸°
compressor = get_compressor()  # ì˜ˆ: LLMChainExtractor.from_llm(llm)

# âœ… ì••ì¶• retrieverë¡œ ê°ì‹¸ê¸°
compressed_retriever = ContextualCompressionRetriever(
    base_retriever=mutli_query_retriever, base_compressor=compressor
)

rag_chain = (
    {
        "context": compressed_retriever | format_docs,
        "question": RunnablePassthrough(),
    }
    | prompt
    | llm
    | StrOutputParser()
)
result = rag_chain.batch(questions)
for i, ans in enumerate(result):
    print(f"Q : {questions[i]}")
    print(f"A : {ans}")
    print("=" * 50)
